% -------------------------------------------------------------
%  RESUMOS
\setlength{\absparsep}{18pt} % ajusta o espaçamento dos parágrafos do resumo
% -------------------------------------------------------------
% ATENÇÃO: o ambiente 'otherlanguage*' deve ser usado para o resumo que não está na
% língua vernácula do trabalho, com a respectiva opção linguística do pacote 'babel'.
% -------------------------------------------------------------
% resumo em PORTUGUÊS (OBRIGATÓRIO)
\begin{center}
    \textbf{RESUMO}
\end{center}

\begin{otherlanguage*}{brazil}
    Este trabalho investiga diferentes formas de estruturação do problema de 
    Geração Procedural de Conteúdo (PCG) utilizando algoritmos de Aprendizado por 
    Reforço (RL), com foco na criação de fases em formato de labirinto. Foram 
    exploradas múltiplas representações do espaço de estados e ações, diferentes 
    configurações de recompensas e cenários com variações de parâmetros ambientais. 
    O objetivo foi compreender como essas decisões de modelagem influenciam o 
    comportamento e desempenho dos agentes geradores. Para isso, foram implementados 
    experimentos com o algoritmo Proximal Policy Optimization (PPO), analisando sua 
    capacidade de aprender padrões válidos e diversificados de geração, mesmo sem 
    convergir para uma política ótima. Os resultados demonstram que a escolha da 
    representação e da recompensa impacta diretamente na qualidade e diversidade dos 
    conteúdos gerados, sendo também um desafio traduzir percepções humanas visuais em 
    métricas computacionais claras. A pesquisa contribui para a compreensão das 
    limitações e possibilidades do uso de RL no contexto de PCG, oferecendo direções 
    para futuros estudos que visem otimizar o processo de geração de conteúdo em jogos 
    digitais.

    \textbf{Palavras-chave}: RL. PCG. Geração procedural de conteúdo em jogos.
\end{otherlanguage*}
% -------------------------------------------------------------
% -------------------------------------------------------------
% resumo em INGLÊS (OBRIGATÓRIO)
\newpage
\begin{center}
    \textbf{ABSTRACT}
\end{center}
\begin{otherlanguage*}{english}
    This work investigates different ways of structuring the Procedural 
    Content Generation (PCG) problem using Reinforcement Learning (RL) 
    algorithms, focusing on the creation of maze-like game levels. Multiple 
    representations of the state and action spaces were explored, along 
    with various reward configurations and scenarios with environmental 
    parameter variations. The goal was to understand how these modeling 
    decisions influence the behavior and performance of generative agents. 
    To this end, experiments were conducted using the Proximal Policy Optimization 
    (PPO) algorithm, evaluating its ability to learn valid and diverse generation 
    patterns, even without converging to an optimal policy. The results show that 
    the choice of representation and reward structure directly impacts the quality 
    and diversity of the generated content. Moreover, translating human visual 
    perceptions into clear computational metrics remains a challenge. This 
    research contributes to a better understanding of the limitations and 
    potential of RL in the PCG context, offering directions for future studies 
    aimed at optimizing content generation in digital games.
    
    \textbf{Keywords}: RL. PCG. Procedural content generation in games.
\end{otherlanguage*}
\newpage
% -------------------------------------------------------------