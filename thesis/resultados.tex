\chapter{Resultados}

Para todos os cenários, foi utilizado um total de 100.000 \texttt{timesteps} que representa
a quantidade de iterações totais que serão feitas. Dentro de um episódio, foi definido
uma taxa de mudanças que o agente pode fazer que influencia no número máximo de mudanças
que é calculado pela taxa multiplicado pelo total de células. Além disso, também é calculado um número máximo de 
iterações que pela multiplicação do número máximo de mudanças pela
quantidade de células totais no labirinto. No caso do experimento, uma taxa de mudança fixa
para os experimentos inicias de 0.3 foi definida, resultando em um máximo de 7 mudanças no
labirinto e um total máximo de 252 iterações. É importante ressaltar que um episódio pode não
realizar todas as iterações, e terminar porque atingiu o máximo de mudanças ou
por uma condição de término de episódio por convergência.


\section{Cenário 1}
Neste primeiro cenário testado, o estado inicial do ambiente é sempre o mesmo para 
todos os episódios, sendo representado por um tabuleiro com todas as células vazias,
exceto pela primeira posição, \([0,0]\), que representa o início, e pela última
posição, \([5,5]\) que representa o objetivo. Esse cenário pode ser observado 
pela Figura \ref{c1}

\begin{figure}[htb]
	\caption{\label{c1}Observação inicial fixa do primeiro cenário}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/c1.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\FloatBarrier

Esse cenário apresenta uma possível limitação: como os estados iniciais são sempre os mesmos, 
o agente não é exposto a uma diversidade de situações durante o processo de aprendizado. 
Como consequência, ele tende a memorizar uma solução específica para o problema em vez de aprender uma 
política generalizável. Isso implica que, ao ser inserido em um novo cenário, o agente provavelmente 
não conseguirá adaptar-se ou convergir para uma solução adequada, comprometendo sua capacidade de generalização.

No entanto, esse tipo de configuração controlada pode ser útil em etapas iniciais de experimentação, 
pois permite observar com mais clareza o comportamento do modelo e o impacto dos parâmetros sobre os resultados. 
Em ambientes restritos, é possível realizar análises mais precisas e isolar variáveis, o que contribui para a compreensão 
mais profunda do funcionamento do algoritmo antes de aplicá-lo a situações mais complexas e variáveis.


Nesse cenário, o primeiro teste, considerado o mais simples de todos, é que o agente 
consiga criar um labirinto com um tamanho total de 11 células. Neste caso, o labirinto já
vem pré-definido com uma distância de 10 células entre o início e o final, então o agente,
em teoria, não precisa modificar muito o cenário. Nesse caso, o agente só pode colocar 
células vazias ou paredes, e sua função de recompensa é parametrizada pelo tamanho 
do caminho entre o início e o fim, sendo utilizado a função de recompensa que compara
a diferença entre dois estados.

\begin{figure}[htb]
	\caption{\label{mr_c1r3}Recompensa média ao longo do treino por representação}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/mr_c1r3.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\begin{figure}[htb]
	\caption{\label{el_c1r3}Tamanho total de episódios por representação}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/el_c1r3.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\FloatBarrier

Como é possível observar pelas Figuras \ref{mr_c1r3} e \ref{el_c1r3}, todas as representações tendem a seguir o mesmo comportamento,
com \textit{Wide} tendo um crescimento ao longo do tempo e obtendo, no final, o melhor valor. Entretanto, a
representação \textit{Narrow Fixed} tem um comportamento bastante caótico e tem alta variância. Este tipo de
comportamento é relativamente esperado já que a representação sempre terá um padrão de observação fixo. Isto 
misturado com o fato de que o estado inicial sempre é fixo faz com que o modelo nunca realmente aprenda nada.
A Figura \ref{c1-nf} mostra o labirinto final gerado por esta representação

\begin{figure}[htb]
	\caption{\label{c1-nf}Estado final da representação \textit{Narrow Fixed}}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/c1-nfr3.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\FloatBarrier

Como é possível perceber, como a representação sempre começa pela primeira posição e percorre o ambiente de forma sequencial, 
ela tende a focar nas regiões iniciais do labirinto e a desprezar completamente o restante do espaço. 
Isso ocorre porque o agente, ao explorar e atribuir valores aos estados, atualiza prioritariamente as 
primeiras células que encontra, e raramente alcança ou dá atenção às partes mais distantes.



Este comportamento acaba criando um viés espacial muito forte, onde os estados iniciais são 
atualizados com frequência enquanto os estados mais distantes permanecem praticamente inalterados. 
Isso pode ser observado claramente pela Figura \ref{c1-nf}: as células localizadas na primeira e segunda fileiras 
tiveram seus valores significativamente modificados, enquanto o restante do labirinto manteve-se  
inalterado, como se fosse invisível para o agente.

Tal limitação compromete a generalização da política aprendida, uma vez que o agente se torna 
especializado em uma região muito restrita do ambiente. Além disso, esse tipo de representação 
impede uma exploração eficaz e dificulta a aprendizagem de trajetórias que envolvem 
regiões mais profundas e complexas do labirinto.

Diante desses problemas, essa abordagem de representação não é adequada para este tipo de ambiente. 
Por isso, a partir deste ponto, foi optado por não utilizar mais essa representação nas próximas análises e experimentos. 



Após estes testes, foi feito outro muito parecido, mas considerando um novo 
parâmetro na recompensa que é o número de paredes. Assim, o agente é estimulado 
a inserir paredes, de certa forma o guiando para maximizar a verdadeira função objetivo
que é aumentar o caminho entre o início e o objetivo

\begin{figure}[htb]
	\caption{\label{mr_c1r3_1}Recompensa média ao longo do treino por representação, considerando a quantidade de paredes}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/mr_c1r3_1.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\begin{figure}[htb]
	\caption{\label{el_c1r3_1}Tamanho total de episódios por representação, considerando a quantidade de paredes}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/el_c1r3_1.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\FloatBarrier


Como é possível perceber pelas Figuras \ref{mr_c1r3_1} e \ref{el_c1r3_1}, 
a introdução da nova métrica ajudou bastante na convergência do algoritmo, 
significando que as duas métricas estão complementando uma as outras. 
Também é possível perceber que o número total de episódios aumentou, o que pode ser algo bom, pois mostra que o agente
está interagindo bastante com o ambiente e realizando mudanças ou ele consegue atingir as condições necessárias de término.

Para verificar a fundo, os valores das métricas e as recompensas, foi feito um teste com um total de mil episódios, 
considerando o melhor modelo encontrado no cenário de treino, das representações \textit{Wide} e \textit{Narrow Random}.
Como utilizar o tamanho do caminho em conjunto com o número de paredes obteve o melhor resultado, este cenário foi analisado
em maior profundidade.

Como é possível perceber pelas Figuras \ref{dd_c1r3_1} e \ref{dd_nrc1r3_1}, a representação \textit{Wide} teve sucesso em todos os cenários de treino, algo que parece bom, mas 
na verdade só enfatiza o problema anteriormente mencionado, dessa representação tender a decorar somente um tipo de estado 
possível, já que não existe nenhum fator de aleatoriedade inserido. Já a representação \textit{Narrow Random} obteve uma 
grande variedade de resultados, destacando-o fato de que mesmo possuindo valores de recompensa baixos, ela ainda consegue
criar labirintos e não tende a produzir unicamente estados impossíveis.

\begin{figure}[htb]
	\caption{\label{dd_c1r3_1}Distribuição de variáveis da representação \textit{Wide}}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/dd_c1r3_1.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\begin{figure}[htb]
	\caption{\label{dd_nrc1r3_1}Distribuição de variáveis da representação \textit{Narrow Random}}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/dd_nrc1r3_1.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\FloatBarrier



A Figura \ref{c1_end} apresenta um exemplo de estado final de um episódio das duas representações \textit{Narrow Random} e \textit{Wide},
considerando os dois exemplos: com o caminho entre início e fim considerado isoladamente e 
considerado este em conjunto com a métrica de quantidade de paredes.
\begin{figure}[htb]
	\caption{\label{c1_end}Exemplos de estados finais}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/c1-end.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\FloatBarrier
É importante ressaltar que como \textit{Wide} tem sempre controle total das posições, ele vai aprender a decorar um padrão, pois o estado
inicial é sempre o mesmo, um comportamento que não ocorre quando se considera \textit{Narrow Random}, já que apesar de o estado inicial 
nunca mudar, a representação sempre está vendo um conjunto de células diferentes em cada episódio. Isso demonstra a importância de 
sempre considerar um grau de aleatoriedade na estruturação do problema, mesmo se for no estado inicial ou nas representações
de espaço de ação do agente gerador.



\section{Cenário 2}
Este cenário, possui as mesmas características do anterior, só que agora o objetivo é testar
um estado inicial aleatório, sempre com uma posição inicial e uma final. Uma imagem de uma geração
possível pode ser visualizada pela Figura \ref{c2}

\begin{figure}[htb]
	\caption{\label{c2}Representação de um cenário inicial aleatório}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/c2.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\FloatBarrier

As mesmas métricas do cenário anterior foram testadas, primeiramente utilizando o \textit{path\_length} sozinho e depois
em conjunto com \textit{num\_wall}. O resultado do primeiro experimento pode ser observado pelas Figuras \ref{mr_c2r3}
e \ref{el_c2r3}.

\begin{figure}[htb]
	\caption{\label{mr_c2r3}Recompensa média ao longo do tempo por representação}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/mr_c2r3.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\begin{figure}[htb]
	\caption{\label{el_c2r3}Tamanho total de episódios por representação}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/el_c2r3.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\FloatBarrier

O algoritmo apresenta um comportamento semelhante ao apresentado no cenário anterior em questão de convergência e valores
obtidos de retorno. Isso mostra que mesmo introduzindo aleatoriedade, a representação \textit{Wide} conseguiu se adaptar
bem e produzir bons resultados.


O teste com as duas métricas em conjunto pode ser visualizado nas Figuras 
\ref{mr_c2r3_1} e \ref{el_c2r3_1}, que mostram, respectivamente, a 
recompensa média ao longo do tempo e o tamanho total de episódios por representação.
\begin{figure}[htb]
	\caption{\label{mr_c2r3_1}Recompensa média ao longo do tempo por representação}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/mr_c2r3_1.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\begin{figure}[htb]
	\caption{\label{el_c2r3_1}Tamanho total de episódios por representação}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/el_c2r3_1.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\FloatBarrier

Pela análise destes gráficos, é possível observar que a 
introdução de uma métrica nova, também auxiliou no cenário 
onde todos os estados iniciais são gerados aleatoriamente. Para analisar as métricas, foram testados mil épocas da
representação \textit{Wide} com os valores de recompensa considerando as duas métricas.

\begin{figure}[htb]
	\caption{\label{dd_wrc2r3_1}Distribuição de variáveis da representação Wide}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/dd_wrc2r3_1.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\FloatBarrier

Como é possível observar pela Figura \ref{dd_wrc2r3_1}, aconteceu um fenômeno diferente dos observados até agora: 
a grande maioria dos episódios
atingiu o término pelo limite máximo de iterações possíveis.
Isso significa que o agente não está mais realizando mudanças no ambiente, 
o que pode indicar um problema clássico de exploração versus explotação. 
Nesse cenário, o agente pode ter adotado uma política subótima, levando-o a estagnar em um comportamento pouco eficiente. 
Além disso, é possível que ele tenha aprendido que realizar muitas mudanças tende a resultar em recompensas menores, 
o que o leva a preferir não agir e, assim, evitar penalidades.

É importante considerar que um agente não alterar o estados de um labirinto entre 
duas iterações pode ser algo ruim. Se não houver nenhum custo para ficar 
parado ou inalterado, e as mudanças tendem a penalizar ou não recompensar o agente, 
o caminho de menor resistência é não fazer nada. Isso gera uma estagnação, e uma 
convergência. Quando o gerador aprende a não mudar nada para minimizar consequências 
negativas, ele está basicamente "jogando seguro" — o que é um comportamento comum em 
RL quando a função de recompensa não incentiva exploração ou mudança. 


Apesar disso, é possível observar que só em torno de 13,7\% dos casos o agente acaba modificando o labirinto
e tornando-o inviável. O comportamento observado aqui exemplifica que só se basear em funções de valor para
estimar a performance de um modelo não é algo desejável, já que o agente pode ter internalizado uma política
sub-ótima que dificultaria sua performance em generalizações.

A Figura \ref{c2-end} apresenta um exemplo de estado final de um episódio das duas representações 
\textit{Narrow Random} e \textit{Wide},
considerando os dois exemplos: com o caminho entre início e fim considerado isoladamente e 
considerado este em conjunto com a métrica de quantidade de paredes.

\begin{figure}[htb]
	\caption{\label{c2-end}Exemplos de estados finais}
	\begin{center}
	    \includegraphics[scale=0.8]{imagens/c2-end.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\FloatBarrier

Foram considerados várias amostras da representação \textit{Wide} mas foi percebido que elas tendem a realizar poucas ações
no ambiente e depois congelam. Além disso, as ações que realizam parecem não representar mudanças muito efetivas, no
contexto da criação e execução da fase. Em contrapartida, a representação \textit{Narrow Random} parece capturar muito bem
uma tendência em mudar células em torno do início ou objetivo, de forma a tornar o caminho mais comprido. O gráfico abaixo
foi feito para comparar os tamanhos de caminhos, entre as duas representações, sendo a \textit{Narrow Random} considerado
as duas possíveis recompensas.

\begin{figure}[htb]
	\caption{\label{dd_nrc2}Distribuição de \textit{path\_length} na \textit{Narrow Random}}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/dd_nrc2.png}
	\end{center}
	\legend{Fonte: Elaboração própria}
\end{figure}

\FloatBarrier
Com a análise da Figura \ref{dd_nrc2}, é perceptível que, apesar das diferenças notáveis do ponto de vista visual, a comparação 
entre as distribuições de caminhos geradas por ambas as representações revela uma 
variação pouco significativa em termos quantitativos. Esse resultado evidencia uma 
das principais dificuldades enfrentadas no uso de aprendizado por reforço no contexto 
da geração procedural de conteúdo: a complexidade de se modelar, de forma computacional, 
percepções humanas subjetivas. Em outras palavras, nem sempre é trivial traduzir elementos 
visuais, estéticos ou emocionais, inerentemente humanos, em representações que possam ser 
compreendidas e abstraídas por um agente gerador. Essa limitação ressalta o desafio de alinhar 
métricas computacionais com interpretações qualitativas sutis e dependentes de um maior contexto.



\section{Cenário 3}
Neste cenário, uma geração aleatória é utilizada baseada na seguinte probabilidade:
\begin{itemize}
    \item Célula vazia: 70\%;
    \item Célula de parede: 20\%;
    \item Célula de início: 5\%;
    \item Célula de fim: 5\%.
\end{itemize}
A distribuição de probabilidades visa, portanto, equilibrar explorabilidade e complexidade: 
uma grande quantidade de células vazias facilita a navegação do agente nos estágios iniciais do aprendizado,
enquanto uma porcentagem controlada de paredes introduz obstáculos e variações estruturais que promovem a generalização da política.
Se fosse definido uma geração totalmente aleatória, o algoritmo demoraria muito para convergir, pois teria que lidar com muitos
cenários que não fazem sentindo, podendo até mesmo prejudicar no seu aprendizado. Um exemplo de cenário pode ser visualizado pela
Figura \ref{c3}:

\begin{figure}[htb]
	\caption{\label{c3}Representação de um cenário inicial aleatório}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/c3.png}
	\end{center}
	\legend{Fonte: Elaboração própria}
\end{figure}

\FloatBarrier

O primeiro teste nesse ambiente foi considerando que o agente pode apenas alterar células do tipo de início e fim.
O objetivo disso é fazer com que o agente aprende a balancear um ambiente caótico e impossível de ser resolvido 
em um ambiente estável. Nos testes anteriores, o agente demonstrou certa aprendizado em não alterar células de 
início e fim, e só mudar paredes, já que esse comportamento gera uma recompensa. Porém o cenário descrito é simples,
com apenas uma célula de classes não predominantes, então no cenário randômico, a intenção é o agente aprender 
a lidar com um número de parâmetros variáveis. Para realização deste teste, é utilizado uma função que compara 
valores antigos com os novos, e recompensa um total de células de tipo início e fim que se aproxima de um.
As Figuras \ref{s_cr} e \ref{e_cr} apresentam a comparação da distribuição de células de início e fim entre as duas
representações.

\begin{figure}[htb]
	\caption{\label{s_cr}Distribuição de células do tipo \textit{start} nas representações }
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/s_cr.png}
	\end{center}
	\legend{Fonte: Elaboração própria}
\end{figure}

\begin{figure}[htb]
	\caption{\label{e_cr}Distribuição de células do tipo \textit{end} nas representações }
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/e_cr.png}
	\end{center}
	\legend{Fonte: Elaboração própria}
\end{figure}


\FloatBarrier

Como é possível perceber, pela análise das Figuras \ref{s_cr} e \ref{e_cr}, a representação \textit{Narrow Random} apresenta um resultado
muito ruim quando considerados as células de início, com variações muito altas que fogem do valor otimizado. A outra
representação possui valores menos variados, mas ainda assim, não tem uma performance considerada satisfatória. 

Um gráfico para analisar as recompensas no treino pode ser observado abaixo, 
permitindo uma visão mais clara do comportamento do agente ao longo dos episódios e de 
como ele responde às recompensas atribuídas pelo ambiente. 
\begin{figure}[htb]
	\caption{\label{se}Recompensas médias ao longo do tempo nas duas representações}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/se.png}
	\end{center}
	\legend{Fonte: Elaboração própria}
\end{figure}

\FloatBarrier

Analisando a Figura \ref{se}, observa-se um comportamento inesperado em relação ao que havia sido identificado 
anteriormente: a representação \textit{Narrow Random} apresentou resultados superiores, 
com recompensas médias mais elevadas. Isso contrasta com o desempenho ruim anteriormente observado, 
especialmente no que diz respeito às células de início, onde apresentava grande variação e distanciamento 
dos valores ideais. Esse contraste sugere que, apesar de suas limitações em aspectos estruturais específicos, 
a \textit{Narrow Random} pode favorecer uma maior diversidade de trajetórias e estratégias, 
resultando em melhores recompensas acumuladas ao longo do tempo em determinados contextos.

Isso evidencia um dos principais desafios no contexto de PCG, na qual as funções de recompensa nem sempre são facilmente interpretáveis. 
Embora a análise visual do layout do labirinto permita uma compreensão mais intuitiva do comportamento do modelo, 
a avaliação baseada exclusivamente em métricas estatísticas pode levar a interpretações equivocadas. 
Essa discrepância ressalta a importância de combinar abordagens quantitativas e qualitativas na 
análise do desempenho de agentes em ambientes gerados proceduralmente.


O segundo teste neste cenário, é testar um cenário complexo, no qual o agente pode alterar todos os tipos de células, tendo 
o máximo de controle possível sobre o ambiente. Nesse exemplo, foram considerados como métricas importantes o tamanho do caminho
do início até o fím, o número de paredes, de células de início e de fim. O peso das paredes foi definido para tres para
tentar auxiliar na convergência do agente. Os dados de treino, podem ser visualizados abaixo:

\begin{figure}[htb]
	\caption{\label{scenario4}Recompensas médias ao longo do tempo nas duas representações}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/scenario4.png}
	\end{center}
	\legend{Fonte: Elaboração própria}
\end{figure}

\FloatBarrier

Pela análise da Figura \ref{scenario4}, tem se a impressão de que o algoritmo esta convergindo, pois a recompensa esta aumentando a medida do
tempo e se estabilizando em valores não tão distantes em relação aos observados anteriormente. Porém, ao analisar mais a fundo, com
o treinamento do melhor modelo encontrado, foi descoberto um comportamento longe do ideal

\begin{figure}[htb]
	\caption{\label{r4_pl}Distribuição do tamanho do caminho nas duas representações}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/r4_pl.png}
	\end{center}
	\legend{Fonte: Elaboração própria}
\end{figure}

\FloatBarrier

Como é possível perceber pela Figura \ref{r4_pl}, mais de 90\% dos labirintos gerados são considerados inválidos. Isso ressalta outro 
problema de depender somente da análise das recompensas ao longo do tempo, pois mesmo que o agente pareça convergir, ele pode, 
na verdade, estar apresentando um comportamento muito ruim. O principal motivo pela função de recompensa apresentar valores, que 
podem não ser considerados ruins de primeira vista, é a grande complexidade que é gerada ao usar de um ambiente com muitas variáveis.
Quanto mais se adiciona variáveis, maior a chance de elas acabarem tendo uma certa dependência e correlação entre si, tornando o processo
de modelagem de função de recompensa algo muito complexo que raramente irá representar resultados ótimos. A Figura \ref{r4_nwall} 
está representando a distribuição de \texttt{path\_length}.

\begin{figure}[htb]
	\caption{\label{r4_nwall}Distribuição do número de paredes nas duas representações}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/r4_nwall.png}
	\end{center}
	\legend{Fonte: Elaboração própria}
\end{figure}

\FloatBarrier

Como é de se esperar, a função apresenta uma distribuição gaussiana dos valores de paredes, pois foi definido um peso priorizando elas.
Muito provavelmente, a função aprendeu a colocar paredes para obter melhores recompensa, e não está se importando em arrumar a quantidade
de células de início e fim, já que irão garantir menos recompensa. Entretanto, manter todas as células com o mesmo peso, faz com que o modelo
não saiba para qual métrica dar mais "atenção", resultando em um modelo que converge muito mais lento pois precisará explorar muito mais.
Foi feito um teste para avaliar o impacto em equalizar o peso entre todas as métricas:

\begin{figure}[htb]
	\caption{\label{r5-X}Recompensas médias ao longo do tempo}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/r5-X.png}
	\end{center}
	\legend{Fonte: Elaboração própria}
\end{figure}

\begin{figure}[htb]
	\caption{\label{w5-x}Distribuição de métricas ao equalizar os pesos}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/w5-x.png}
	\end{center}
	\legend{Fonte: Elaboração própria}
\end{figure}

\FloatBarrier
Comparando os resultados observados nas Figuras \ref{r5-X} e \ref{w5-x}, 
deixar todos os pesos iguais não surtiu em nenhum efeito, considerando 
a o número de caminhos inválidos gerados e a distribuição do número de paredes. Isso exemplifica um dos grandes
problemas relacionados a utilização de recompensas ponderas, já que é difícil estimar um valor que tenha impacto 
o suficiente, sem atrapalhar na convergência e resultados finais do gerador. Além disso, a escolha de qual 
métrica utilizar na ponderação, pode não ser clara, dependendo de vários testes extensivos para estimação de comportamento,
já que as variáveis de um problema podem ter comportamentos exclusivos ou inclusivos e o gerador pode interpretar
esses comportamentos de maneira não padronizada.


\section{Taxa de mudança}
Para testar a influência da taxa de mudança, que controla o quanto o agente pode mudar o ambiente, foi utilizado o "cenário 2" e 
a representação do tipo \textit{Narrow Random} com duas métricas de recompensa, por obter bons resultados nos experimentos realizados.
Como todos os experimentos foram realizadas com uma taxa relativamente baixa de 0.3, será 
proposto a utilização de um valor muito baixo de 0.1, um valor intermediário de 0.5 e um valor alto 
de 1 que simboliza o maior tipo de mudança. A Figura \ref{change_rate} mostra a recompensa média por tempo em relação a taxa de mudança
selecionada.

\begin{figure}[htb]
	\caption{\label{change_rate}Recompensas médias ao longo do tempo}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/change_rate.png}
	\end{center}
	\legend{Fonte: Elaboração própria}
\end{figure}

\FloatBarrier

O gráfico acima ilustra muito bem, como é importante considerar de maneira cuidadosa os valores de taxa de mudança que um agente 
pode realizar em um ambiente. Valores altos, fazem com que o agente sempre queira ficar alterando algo, fazendo com que o 
espaço de observações seja transformado muitas vezes durante o episódio, causando instabilidade nas recompensas e 
dificuldade na convergência. Isto é
exemplificado no gráfico, onde inicialmente existe um leve crescimento, mas o algoritmo começa a performar mal e 
seu desempenho cai, algo muito raro de acontecer, já que geralmente o agente converge para um valor, mesmo que não seja
o ótimo. Já uma taxa de mudança baixa obteve um ótimo desempenho com valores convergindo para o melhor cenário possível,
mostrando que nesse cenário, quanto mais o agente muda o ambiente, pior serão as recompensas obtidas. É importante ressaltar
que essa mudança vária de acordo com a complexidade do caso que está sendo analisado, já que, em jogos mais complexos, pode
ser necessário um maior conjunto de ações para alcançar um estado desejável. A Figura \ref{cr01metrics} analisa em
maior detalhe algumas métricas de desempenho considerando uma taxa de mudança de 0.1.
\begin{figure}[htb]
	\caption{\label{cr01metrics}Métricas avaliadas do modelo com taxa de mudança de 0.1}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/cr01metrics.png}
	\end{center}
	\legend{Fonte: Elaboração própria}
\end{figure}

\FloatBarrier
Como é possível observar pela figura acima, nem sempre um modelo que está convergindo para valores
muito próximos de zero terá um resultado ótimo. Apesar de o agente conseguir criar, na grande maioria dos casos, 
caminhos viáveis no labirinto, ele sempre está realizando três ações, que é o total possível nesse cenário. Esse fato,
mostra que a taxa escolhida poderia ser um pouco melhor, pois pelo fato de o agente sempre escolher realizar o mesmo
número máximo de ações significa que ele "enxerga"\ estas ações como valiosas e impactantes para maximizar a recompensa,
então realizar mais ações provavelmente traria ainda mais valores de retorno positivo.









\section{Problemas encontrados}

\subsection{Recompensa}
Durante o desenvolvimento e a realização dos testes, foi possível observar que a definição das funções de recompensa 
representa um dos aspectos mais sensíveis e desafiadores do processo de aprendizado por reforço. 
Em ambientes de geração procedural, por exemplo, a construção de recompensas baseadas apenas em uma métrica isolada, 
como o comprimento do caminho em um labirinto, pode se mostrar ineficaz. Embora tal métrica possa, em tese, 
incentivar o agente a produzir mapas mais interessantes ou desafiadores, ela não é suficiente para capturar 
a complexidade do cenário. Isso ocorre porque o comprimento do caminho é, frequentemente, influenciado por 
outras variáveis estruturais, como o número e a posição das paredes. Assim, o agente pode acabar otimizando 
aspectos colaterais que não representam diretamente o objetivo desejado, dificultando a convergência e levando 
a comportamentos sub-ótimos.

Por outro lado, incluir múltiplos componentes de recompensa, como a quantidade de células de início e fim, 
também apresenta desafios. Quando muitas recompensas são combinadas, o agente pode ter dificuldade em determinar 
quais delas deve priorizar, levando a um aprendizado ambíguo ou até mesmo contraditório. Isso pode 
obfuscar o verdadeiro objetivo do ambiente, impedindo que o comportamento aprendido seja consistente ou generalizável.

Adicionalmente, recompensas mal definidas ou fortemente correlacionadas (i.e., variáveis dependentes) 
podem interferir negativamente no aprendizado, já que modificações em uma métrica podem afetar inadvertidamente outras. 
Esse tipo de sobreposição pode levar o agente a interpretar sinais de maneira errada, ou explorar caminhos de otimização não intencionais.



Outro ponto crítico é a definição dos pesos associados a cada componente da função de recompensa. 
Ajustar esses pesos é uma tarefa não trivial e, muitas vezes, altamente empírica. Pequenas variações 
podem induzir prioridades indesejadas, fazendo com que o agente desenvolva comportamentos que maximizam 
recompensas irrelevantes ou até prejudiciais à qualidade do mapa gerado. Em ambientes complexos e 
com múltiplos objetivos, encontrar uma combinação de pesos que represente fielmente as intenções 
do projetista do ambiente é um grande desafio, frequentemente exigindo muitas iterações e testes manuais.



Esses desafios na definição e balanceamento das funções de recompensa evidenciam que a aplicação de 
algoritmos de Reinforcement Learning para tarefas de geração procedural de conteúdo pode não ser, necessariamente, 
a abordagem mais prática ou eficiente no contexto do desenvolvimento de jogos. Isso se deve ao fato de que esses ambientes 
exigem um grau elevado de precisão na formulação do problema e um entendimento aprofundado sobre a influência de múltiplas 
variáveis no comportamento do agente. No entanto, na prática, desenvolvedores de jogos nem sempre dispõem do conhecimento 
técnico necessário para elaborar funções de recompensa bem calibradas ou formalizar com clareza os objetivos desejados em termos 
computacionais.

Consequentemente, mesmo que um determinado algoritmo de RL demonstre excelente desempenho em estudos acadêmicos ou benchmarks controlados, 
sua replicação por profissionais da indústria pode se mostrar limitada. Isso ocorre não por deficiência do algoritmo em si, mas 
pela dificuldade prática de traduzir objetivos subjetivos, como jogabilidade, diversão ou desafio, em métricas formais que 
conduzam o agente ao comportamento desejado. Este problema mencionado, tende a se evidenciar ainda mais quanto
maior a complexidade de um jogo, em relação a quantidade de variáveis que uma fase pode conter. Assim, o sucesso 
da aplicação de RL nesse domínio depende não apenas da escolha do 
algoritmo, mas também da capacidade de modelagem do problema, o que pode representar uma barreira significativa fora do 
ambiente de pesquisa.

\subsection{Representação}
Um dos principais desafios enfrentados durante o desenvolvimento deste trabalho foi a representação adequada 
do espaço de estados e ações no contexto da geração procedural de fases. A tarefa de geração de conteúdo em jogos, 
especialmente em ambientes como labirintos ou plataformas, envolve uma grande variedade de elementos e estruturas possíveis. 
Essa diversidade resulta em espaços de ação e observação altamente dimensionais, o que dificulta significativamente o 
processo de aprendizado por reforço.

Como tentativa de mitigar esse problema, foram testadas diferentes formas de modelagem do agente, 
alterando a maneira como ele percebe o ambiente e decide suas ações. No entanto, mesmo com essas adaptações, a 
definição de uma representação que fosse ao mesmo tempo expressiva e eficiente mostrou-se desafiadora. 
A depender da estruturação do problema, é possível criar agentes que demonstram bom desempenho em executar 
determinados tipos de ações ou em operar dentro de padrões específicos do espaço de estados. 
No entanto, ao se empregar múltiplas formas de representação, pode ocorrer sobreposição ou
divergência significativa entre os comportamentos dos agentes. Em alguns casos, diferentes representações 
produzem agentes com métricas e comportamentos muito semelhantes; em outros, os resultados podem variar drasticamente. 
Essa inconsistência dificulta a realização de comparações diretas, comprometendo a análise objetiva sobre a 
eficácia de cada modelagem. Esse cenário evidencia a complexidade envolvida na escolha da representação ideal e 
ressalta a importância de métodos mais padronizados ou de métricas que capturem melhor as nuances entre os diferentes agentes.

Sendo assim, o desalinhamento entre a complexidade do espaço e a capacidade de generalização do agente impacta 
diretamente na qualidade do conteúdo gerado, tornando o processo de treinamento mais demorado e instável. 
Dessa forma, a representação do espaço permanece como um dos principais pontos críticos na aplicação de RL para PCG, 
exigindo abordagens mais sofisticadas e, possivelmente, métodos híbridos que integrem conhecimento prévio ou aprendizado 
supervisionado para guiar o processo de modelagem.

\section{Considerações Finais}
Embora nenhum dos experimentos realizados neste trabalho tenha convergido para uma política ótima que maximize 
consistentemente a recompensa total, isso não invalida o uso de algoritmos de aprendizado por reforço, 
como o Proximal Policy Optimization (PPO), em tarefas de geração procedural de conteúdo em jogos digitais. 
Os resultados obtidos indicam que, mesmo diante da ausência de uma convergência clara, o agente ainda foi 
capaz de produzir conteúdos válidos, neste caso, labirintos jogáveis e estruturalmente coerentes o que 
demonstra a viabilidade de tais abordagens mesmo em cenários de alta complexidade.

É importante destacar que as funções de recompensa utilizadas foram cuidadosamente elaboradas com o 
intuito de induzir o agente a maximizar métricas específicas relacionadas à qualidade do conteúdo gerado. 
Entretanto, a formulação dessas recompensas pode ter limitado a capacidade de generalização e adaptação do agente, 
evidenciando a sensibilidade dos algoritmos de RL à definição dessas funções. Ainda assim, os modelos 
treinados apresentam potencial como geradores pseudoaleatórios, que, embora não alcancem soluções ótimas, 
aprendem a evitar casos extremos ou inválidos ao internalizar parcialmente os critérios estabelecidos pelas 
recompensas durante o processo de otimização.

Outro ponto crítico observado ao longo dos experimentos é a elevada quantidade de hiperparâmetros 
e decisões de modelagem envolvidos na estruturação do problema. Aspectos como o formato do espaço de observação, 
a granularidade das ações, os tipos de cenários utilizados nos testes e a própria taxa de atualização do ambiente 
influenciam diretamente o desempenho e a estabilidade do treinamento. Nesse sentido, a decomposição do 
problema em subcomponentes mais simples, cada um com objetivos mais específicos e mensuráveis,
pode representar uma estratégia eficiente para mitigar a complexidade envolvida.

Além disso, abordagens baseadas em \textit{Meta-Learning}, que visam permitir que o próprio sistema aprenda a 
otimizar seus hiperparâmetros ou estrutura interna, podem desempenhar um papel fundamental na automação e 
aprimoramento desses processos. Tais abordagens não apenas facilitam a adaptação a diferentes domínios, 
como também reduzem o custo cognitivo e computacional associado à experimentação manual intensiva.

Como trabalhos futuros, é recomendável explorar técnicas que reduzam a alta dimensionalidade dos espaços 
de ação e observação, um dos principais desafios enfrentados neste tipo de tarefa. 
Estratégias como a decomposição do processo de geração em subetapas mais simples podem facilitar o 
aprendizado, além de permitir maior controle sobre aspectos específicos do conteúdo gerado. 
A integração de abordagens de \textit{multi-task learning} também se apresenta como uma alternativa promissora, 
possibilitando que o agente aprenda simultaneamente múltiplas habilidades relacionadas à geração de conteúdo. 
Por fim, sugere-se a investigação de algoritmos híbridos que combinem aprendizado por reforço com métodos 
baseados em dados, como \textit{imitation learning} ou \textit{offline RL}, a fim de acelerar a convergência 
e melhorar a qualidade do conteúdo gerado mesmo em contextos com sinais de recompensa esparsos.




