\chapter{Referenciais teóricos}

\section{Geração Procedural de Conteúdo}
Geração procedural de conteúdo (PCG) em jogos refere-se à criação de conteúdo do jogo de forma automática
utilizando algoritmos \cite{Togelius}. Essa abordagem busca reduzir o trabalho manual exigido para criar 
níveis, personagens, mapas, itens ou até mesmo regras do jogo, oferecendo ao mesmo tempo variedade, 
repetibilidade e adaptação ao jogador. Segundo \citeonline{Hendrikx}, técnicas procedurais são uma alternativa para 
criar mundos de jogo complexos em um tempo limitado, sem impor uma grande carga de trabalho aos designers de conteúdo do jogo.

Dentro de um jogo, existem inúmeras esferas onde a utilização de algoritmos de PCG tem potencial para ser explorada. 
\citeonline{Hendrikx} divide em seu estudo seis diferentes classes na qual seus conteúdos podem ser gerados de forma procedural. 
A primeira são os \textit{Game Bits} são unidades elementares de conteúdo dentro de um jogo, que tipicamente não engajam com o usuário 
quando consideradas independentemente, como por exemplo, sons, texturas, vegetação e elementos de natureza. O \textit{Game Space} é o 
espaço onde ocorre o jogo e são encontrados os \textit{Game Bits}. Já os \textit{Game Systems} são os sistemas que governam como os elementos 
de um jogo vão fluir e interagir entre si, tornando o jogo mais apelativo e fiel a realidade, como por exemplo, o comportamento 
das entidades, as redes de tráfego e os sistemas urbanos. O \textit{Game Scenario} caracteriza, muitas vezes de forma transparente 
para o usuário, a ordem e como os eventos do jogo se desenrolam, como por exemplo, a presença de história e a criação de fases.
O \textit{Game Design} é responsável pelas regras e parâmetros que influenciam como o jogo vai funcionar, como por exemplo, a definição 
do que o jogador pode fazer e quais são os seus objetivos. Por fim, o \textit{Derived Content} é todo o conteúdo que é criado como um 
subproduto do mundo de um jogo, aumentando a imersão do jogador, como por exemplo, tabelas de classificação de pontuação dos jogadores.


Mesmo com essas amplas esferas definidas, ainda persiste um certo limiar de confusão sobre quais algoritmos podem ser
considerados de Geração Procedural de Conteúdo, devido à vasta gama de técnicas e contextos existentes. 
\citeonline{Togelius} afirma que seria inútil esperar uma definição de geração procedural de conteúdo em jogos 
que seja unânime entre todos. 

Um exemplo disso é a discussão sobre a necessidade de esses algoritmos serem necessariamente aleatórios ou 
não: geralmente, os geradores incluem algum grau de aleatoriedade, pois há restrições rigorosas quanto aos 
tipos de conteúdo que podem ser criados. Dentro dessas limitações, no entanto, o conteúdo gerado pode variar 
de forma pseudoaleatória. Essa característica está presente na maioria das implementações de Geração 
Procedural de Conteúdo (PCG) conhecidas, combinando controle e diversidade para produzir resultados variados e adequados ao contexto.

\section{Redes Neurais}
As redes neurais são modelos computacionais pertencentes a área de aprendizado de máquina, que são 
inspiradas pela estrutura e funcionamento dos neurônios de um cérebro, e como estes processam as informações.

Uma rede neural consiste de uma vasta rede interconectada de estruturadas menores, denominadas \textit{Perceptrons}, conhecidas
como neurônios no contexto de \textit{Deep Learning}.
Estas estruturas são responsáveis por receber vários dados de entrada (valores numéricos), e realizar uma soma ponderada destes 
valores com pesos \(w_{n}\), produzir uma saída, que, somada com um valor de vies \(b\), passará por uma função
de ativação \(f_{a}\) para gerar uma saída. Este processo pode ser descrito por
\begin{equation}
    y = \sigma ( \sum_{i=0}^{n}x_{i} \times w_{i} + b)
\end{equation}
\noindent onde
\begin{itemize}
    \item \(y\) representa a saída no \textit{Perceptron};
    \item \(\sigma\) representa uma função de ativação, como por exemplo uma sigmoide;
    \item \(x_{i}\) representa um valor de entrada \(i\);
    \item \(w_{i}\) representa um peso associado ao valor de entrada \(x_{i}\);
    \item \(b\) representa um viés, utilizado para equilibrar a soma ponderada.
\end{itemize}

Os \textit{Perceptrons} utilizam de uma técnica fundamental denominada gradiente descendente, pelo qual 
a rede "aprende", calculando os valores dos pesos.
O objetivo desta técnica é alterar os pesos de forma a minimizar uma função de erro, que quantifica a diferença 
entre a saída prevista pela rede e o valor esperado. Para isso, calcula-se o gradiente da função de erro em relação a cada peso, 
indicando a direção de maior crescimento do erro. A partir disso, os pesos são atualizados no sentido oposto ao gradiente, 
ajustando a rede gradualmente para que produza saídas mais precisas. Esse processo é repetido iterativamente, e, quando aplicado 
a redes neurais com múltiplas camadas, utiliza-se o algoritmo de (\textit{backpropagation}), que permite distribuir o 
erro de forma eficiente por toda a estrutura da rede, garantindo um aprendizado mais eficaz. Em resumo, quanto mais uma rede
está errando, maior será a mudança do peso de seus neurônios, e quanto menor o erro, mais sútil será a penalização.

Quando a saída destes \textit{Perceptrons} é conectado na entrada de um novo, é criado uma rede
\textit{Multilayer Perceptron} (MLP), também conhecido como \textit{Neural Networks}, 
\textit{Deep feedforward networks} ou \textit{feedforward neural networks} \cite{Goodfellow-et-al-2016}.
Estes modelos são chamados de \textit{feedforward}, pois a informação de entrada sempre é passada para o neurônio da
da frente, e assim acontece com as informações processadas por estes neurônios. A conexão de neurônios de uma camada 
com os neurônios de outra camada faz com que uma rede (\textit{network}) seja criada. O conceito de profundidade (\textit{depth})
de uma rede se refere ao número de neurônios em cada camada e também ao número de camadas que uma rede 
possui. A Figura \ref{mlp-simples} ilustra uma rede neural simples, com três entradas, quatro neurônios da primeira camada escondida e 
dois neurônios de saída.

\begin{figure}[htb]
	\caption{\label{mlp-simples}Representação de uma rede MLP simples}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/mlp-simples.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\FloatBarrier

Redes neurais apresentam grande capacidade de aprendizado devido à sua estrutura composta por múltiplas camadas de \textit{Perceptrons}
interligados, capazes de modelar relações complexas entre variáveis. Essa capacidade decorre principalmente da composição sucessiva 
de funções não lineares, o que permite à rede aproximar funções arbitrárias com elevada precisão, mesmo em espaços de alta dimensionalidade.
À medida que se aumenta o número de camadas, a rede se torna capaz de capturar hierarquias de abstração nos dados: 
camadas iniciais tendem a identificar padrões mais simples, enquanto camadas mais profundas extraem características cada vez 
mais complexas e específicas. No entanto, redes muito profundas exigem técnicas específicas de treinamento, 
como normalização, inicialização cuidadosa dos pesos e métodos para evitar o desaparecimento ou explosão de gradientes, 
a fim de manter a estabilidade e a eficiência do aprendizado.



\section{Aprendizado por reforço}
O aprendizado por reforço, \textit{Reinforcement Learning} (RL), surge da ideia inerente de que os
seres humanos aprendem pela interação com o ambiente. Esta ideia pode ser traduzida facilmente para um
ambiente computacional pela representação de agentes, um ambiente onde estão situados e podem interagir,
e as ações que podem realizar. 

Para exemplificar este conceito, é possível utilizar um jogo de xadrez: o agente será
um dos jogadores, o ambiente será o tabuleiro de xadrez e as peças em suas respectivas posições,
e a ação do agente será mover uma peça do tabuleiro para uma determinada posição. É importante
ressaltar que estas definições dependem totalmente do contexto e do objetivo final. Nesse 
mesmo exemplo, uma peça de xadrez poderia ser considerada um agente, e a ação
corresponderia a um conjunto de movimentos possíveis que essa peça pode realizar, 
como mover uma posição para frente, mover uma posição para trás, entre outros.

O aprendizado por reforço pode ser definido brevemente como um método computacional para entender e
automatizar o aprendizado baseado em objetivos e o processo de tomada de decisão.
Este tipo de aprendizado consiste em
aprender qual a melhor ação que deve ser tomada dentro de uma determinada situação de forma
a conseguir a melhor recompensa possível para este cenário. Tradicionalmente, o agente que irá 
aprender não sabe de quais ações deverá tomar, quais serão suas recompensas, e qual será os seus impactos
futuros no ambiente. Em vez disso, o agente precisará interagir com o ambiente várias vezes para encontrar
as melhores ações, por um processo de tentativa-e-erro, e mapear as melhores recompensas considerando
estados futuros.

O aprendizado por reforço é diferente de outros algoritmos tradicionais de IA, como o aprendizado 
supervisionado e não supervisionado, pois o agente não tem acesso a nenhum tipo de dados pré-existentes, 
baseando-se apenas nas próprias experiências que abstrai ao longo do tempo por meio da interação
com o ambiente.


% TODO: Change this line ?
Um dos principais desafios relacionados ao aprendizado por reforço é o compromisso entre exploração 
(\textit{exploration}) e explotação (\textit{exploitation}). Para maximizar a recompensa recebida, 
o agente deve preferir ações que já tentou no passado e que se mostraram eficazes em produzir
boas recompensar. Em contrapartida, para descobrir estas ações, o agente precisa tentar ações que
nunca realizou antes. Em resumo, o agente precisa explotar o que já experimentou para conseguir uma
boa recompensa, mas também explorar para conseguir tomar melhores ações no futuro.

\subsection{Processo de Decisão de Markov}
O aprendizado por reforço é tipicamente modelado na forma de um \textit{Markov decision process} (MDP),
que define a interação entre o agente e o ambiente em termo de estados, ações e recompensas.
O processo pode ser definido pela tupla \(M\) abaixo:
\begin{equation}
    M = (S, A, P_a(s, s'), R_a(s, s'))
\end{equation}

\noindent onde:
\begin{itemize}
    \item \(S\) é o conjunto de estados \(s\) do ambiente;
    \item \(A\) é o conjunto de ações \(a\) do ambiente;
    \item \(P_a(s, s')\) é a probabilidade da ação \(a\) no estado \(s\) em um determinado tempo
        \(t\) produzir um estado \(s'\) no tempo \(t+1\);
    \item \(R_a(s, s')\) é a recompensa imediata recebida após transicionar do estado \(s\) para
        o estado \(s'\) devido a ação \(a\)
\end{itemize}

% TODO: Inserir imagem aqui
Em um determinado tempo \(t\), o agente recebe uma representação do estado do ambiente, \(S_{t}\in S\),
e seleciona uma ação , \(A_{t}\in A.\) Após uma passagem de tempo, como consequência da ação.
escolhida o agente recebe uma recompensa \(R_{t+1}\in R\subset \real \mathbb{R}\).
O processo de decisão, juntamente com o agente, irá produzir uma sequência do tipo
\begin{equation}
    S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3,\ldots
\end{equation}

Isso significado, que dado um estado inicial \(S_0 \in S\) 
o agente irá realizar uma ação \(A_0 \in A\) 
que produzirá uma recompensa \(R_1 \in R\) 
e levará a um novo estado \(S_1 \in S\). Essa cadeia, ocorrerá
até o agente atingir seu objetivo. Em um caso finito de MDP, os conjuntos \(S\), \(A\) e \(R\) terão
um número finito de elementos

É esperado que todos os estados de um MDP possuam a propriedade de Markov, ou seja,
a probabilidade de todos os valores possíveis de \(S_t\) e \(R_t\) dependem apenas do
estado e ação imediatamente anteriores, \(S_{t-1}\) e \(A_{t-1}\). O estado deve incluir
informações sobre todos os aspectos do passado da interação entre o agente e o ambiente.
Esta propriedade pode ser traduzia pela equação abaixo.
\begin{equation}
    P(S_{t+1} = s_{t+1}\mid S_{t} = s_{t}, S_{t-1} = s_{t-1},\ldots, S_0 = s_0) = 
    P(S_{t+1} = s_{t+1}\mid S_{t} = s_{t})
\end{equation}


% SUb?
No aprendizado por reforço, o propósito ou objetivo de um agente é formalizado a partir
de uma recompensa que é passada a ele por meio do ambiente. A cada tempo \(t\), a recompensa
é uma valor \(R_{t} \in \mathbb{R}\). O objetivo do agente é maximizar a quantidade de
recompensas que ele irá receber ao longo do tempo, mais especificamente, maximizar a 
recompensa cumulativa e não imediata. Na forma mais simples, o retorno esperado \(G\) no 
tempo \(t\) será a soma das recompensas
\begin{equation}
    G_{t} \doteq R_{t+1} + R_{t+2} + R_{t+3} + \ldots + R_{T}
\end{equation}

Outra forma mais utilizada para o cálculo do retorno esperado, é utilizando um fator
de desconto
\begin{equation}
    G_{t} \doteq  R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots =
    \sum_{k=0}^{\infty}\gamma^k R_{t+k+1}
\end{equation}
\noindent onde \(\gamma\) é a taxa de desconto, \(0 \leq \gamma \leq 1\).

Se \(\gamma = 0 \) o agente só esta se importando em maximizar a recompensa imediata.
A medida que \(\gamma\) se aproxima de 1, os valores de retorno futuros 
passam a ter maior influência na decisão do agente. 
Em resumo, o fator de desconto \(\gamma\) é útil para controlar o quanto o agente 
valoriza recompensas futuras em relações às imediatas, ou de forma mais intuitiva,
o quão longe no tempo o agente consegue "enxergar".

Existem dois tipos de tarefas do aprendizado por reforço. As tarefas episódicas são aquelas
nas quais a interação entre o agente e o ambiente pode ser descrita em uma sequência 
separada de episódios, onde cada episódio consiste numa sequência finita de tempo e 
possuem um estado final \(s_{T}\), onde \(T\) é o instante de terminação
da tarefa. Já as tarefas contínuas não definem um estado terminal, sendo assim,
\(T\) é infinito.

Os dois grandes pilares do aprendizado por reforço consistem nas funções de valor, 
\textit{value functions}, e políticas, \textit{policy}. As funções de valor estimam
o quão bom é para o agente estar em um determinado estado ou quão bom é executar
uma dada ação em um dado estado. As recompensas que o agente espera receber,
depende das ações que ele escolherá. Deste modo, funções de valor são
definidas de acordo com políticas particulares. 

Uma política \(\pi\) é o mapeamento de estados, \(s \in S\),
e ações, \(a \in A\), em uma probabilidade \(\pi(a \mid s)\) de se escolher uma 
ação \(a\) em um estado \(s\).

A função de valor, popularmente conhecida como \textit{state-value function},
de um estado \(s\), utilizando uma política \(\pi\), denotado
de \(v_{\pi}(s)\) é o retorno esperado quando se começa no estado \(s\) e segue
a política \(\pi\). Esta função é definida pela equação abaixo. 
\begin{equation}
    \label{eq:v_value}
    V_{\pi}(s) = \mathbb{E}_{\pi}[G_{t} \mid S_{t} = s] = \mathbb{E}_{\pi} 
    [\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} \mid S_{t} = s], \text{para todo} s \in S
\end{equation}
\noindent onde \(\mathbb{E}_{\pi}[\cdot]\) é o valor esperado de uma variável aleatória,
seguindo uma política \(\pi\), definido por
\begin{equation}
    \mathbb{E}[G_{t}| S_{t} = s] = \sum_{g}^{} g * Pr(G_{t} = g \mid S_{t} = s)
\end{equation}
Um estado \(s\) e uma ação realizada de acordo com uma política \(\pi(a \mid s)\) 
podem levar a diferentes retornos \(G_{t} = g\). A função de valor \(v_{\pi}(s)\) 
usa as probabilidades de todos os retornos dada um estado, 
estimando assim, o quão bom é estar naquele estado, dado que quanto maior for
o valor de \(v_{\pi}(s)\) maior será a recompensa acumulada ao longos do tempo e dos
episódios daquele estado

De forma similar, o valor da escolha de uma ação \(a\) em um estado \(s\) seguindo uma
política \(\pi\), denotado por \(q_{\pi}(s, a)\) é chamado de função do par 
ação-valor, popularmente como \textit{action-value function}, e é definido pela equação:
\begin{equation}
    \label{eq:q_value}
    Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_{t} \mid S_{t} = s, A_{t} = a] = \mathbb{E}_{\pi} 
    [\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} \mid S_{t} = s, A_{t} = a]
\end{equation}

O termo \(R_{t+k+1}\), presente em (\ref{eq:v_value}) e (\ref{eq:q_value}),
representa a recompensa recebida pelo agente no tempo \(t+k+1\), isto é,
após a realização da ação no tempo \(t+k\). Essa recompensa é um valor real fornecido pelo 
ambiente como consequência da trajetória do agente e não é calculada antecipadamente. No momento da tomada de decisão 
no tempo \(t\), o agente ainda não possui acesso a \(R_{t+k+1}\), pois ela depende de estados e ações futuras. 
Sendo assim, o agente não calcula diretamente esse valor, mas sim estima o retorno esperado a partir do tempo atual, 
baseando-se nas experiências adquiridas em episódios anteriores.

Durante o treinamento, ao interagir com o ambiente, o agente coleta sequências reais de recompensas como
\(R_{t+1}, R_{t+2}, \ldots \), que são utilizadas para atualizar suas estimativas de valor. Essas estimativas 
são então reutilizadas nos episódios seguintes como aproximações dos retornos futuros, ou seja, o agente se apoia 
no conhecimento acumulado para prever quais ações provavelmente resultarão em boas recompensas. Sendo assim,
a expectativa sobre \(R_{t+k+1}\) é construída por meio da generalização de recompensas observadas em interações anteriores.

É importante ressaltar que no início do treinamento, o agente não possui informações sobre os estados futuros 
e nem sobre as recompensas associadas a eles, como \(S_{t+k+1}\).
Por esse motivo, as ações são inicialmente realizadas de forma predominantemente exploratória, 
com o objetivo de coletar dados sobre o comportamento do ambiente. 
À medida que o agente interage com o ambiente e observa as transições e recompensas reais dos episódios, 
ele constrói gradualmente suas estimativas de valor e ajusta sua política de decisão.


O objetivo principal do aprendizado por reforço é encontrar uma política que 
resulta no máximo de retorno, ou seja, mapear a melhor ação em um estado de forma
a conseguir a melhor recompensa. Essa política é denotada como política ótima 
\(\pi^*\). A partir desta definição, é possível definir, a função de valor
de estado ótima, denotada por \(V^*(s)\) e definida como:
\begin{equation}
    V^*(s) \doteq \max_{\pi} V_{\pi}(s)
\end{equation}
Da mesma maneira é possível encontrar a função do par estado-ação ótima \(Q^*(s,a)\),
definida como:
\begin{equation}
    Q^*(s,a) \doteq \max_{\pi} Q_{\pi}(s, a)
\end{equation}

Algoritmos de RL podem usar a política de modo \textit{on-policy} e \textit{off-policy}.
No primeiro método, o agente utiliza da política \(\pi\) tanto para tomar a decisão 
quanto para aprender com os resultados obtidos, atualizada a própria política, 
com base nessas experiências, tendo como exemplo o algoritmo SARSA. Já no outro método, a política
usada para a tomada de decisões \(\mu\) se diferencia da política aplicada para o 
aprendizado \(\pi\), sendo um exemplo clássico, o algoritmo Q-Learning.



\subsection{Q-Learning}
O algoritmo de \textit{Q-Learning} \cite{watkins1992} tem como objetivo aprender uma política ótima 
de tomada de decisão em ambientes estocásticos e potencialmente desconhecidos, pertencendo à classe dos 
métodos \textit{off-policy} baseados em valores. O Q-Learning busca estimar a função de valor-ação. Neste
algoritmo uma função ação-valor \(Q\) é utilizado para seleção da ação, dado um determinado
estado seguindo uma política atual. No entanto, por ser um método \textit{off-policy}, a sua 
atualização segue uma política ótima, sempre utilizando o melhor valor \(Q(s,a)\) para atualização, e desta
forma, o algoritmo sempre vai convergir em direção a \(q_{*}\) \cite{Sutton}
A atualização da função \(Q(s, a)\) é definida pela fórmula abaixo.


\begin{equation}
    Q(s_{t}, a_{t}) := Q(s_{t}, a_{t}) + \alpha * [r_{t} + \gamma * max_a' Q(s_{t+1}, a') - Q(s_{t}, a_{t})]
\end{equation}
onde:
\begin{itemize}
    \item \(Q(s_{t}, a_{t})\) é a estimativa da recompensa esperado, ou seja, o valor de recompensa que o agente
    irá receber ao tomar a ação \(a_{t}\) no estado \(s_{t}\);
    \item \(\alpha\) é a taxa de aprendizado, definindo a influência do novo valor calculado no valor anterior;
    \item \(r_{t}\) é a recompensa imediata recebida após tomar a ação \(a_t\) no estado \(s_{t}\);
    \item \(\gamma\) é o fator de desconto e controla o quanto o agente valoriza recompensas futuras em 
    comparação com as imediatas;
    \item \(max_a' Q(s_{t+1}, a')\) é a melhor estimativa de valor futuro que o agente pode receber no
    próximo estado \(s_{t+1}\), assumindo que ele escolha a melhor ação \(a'\) possível.
\end{itemize}

O objetivo do algoritmo \textit{Q-Learning} é encontrar todos os estados possíveis para certas ações e manter 
um registro deles, utilizando uma tabela armazenando todos os valores Q para cada ação e estado possível. Para cada
ação realizado, uma recompensa será concedida, seja positiva ou negativa, de modo que em visitas subsequentes deste 
mesmo estado, possa ser determinado qual a melhor ação a ser escolhida.

\textit{Q-Learning} é empregado com grande frequência para solução de aprendizagem por reforço, devido a sua simplicidade
e eficiência. Entretanto, quando considerado um espaço grande de ações e observações, o algoritmo apresenta problemas 
de escalabilidade e generalização. Isso ocorre porque a representação da função \(Q(s, a)\)
em tabelas torna-se inviável, tanto em termos de memória quanto de tempo de convergência, exigindo uma quantidade massiva 
de interações para cobrir o espaço de estados de forma adequada. Para lidar com essas limitações, surgiram abordagens 
baseadas em funções aproximadoras, como as redes neurais, que permitem estimar os valores de \(Q\)
de forma contínua e mais eficiente.


\subsection{SARSA}
Um algoritmo semelhante ao \textit{Q-Learning} é o \textit{SARSA}, introduzido por \citeonline{sarsa}, se diferenciando 
na forma como realiza a atualização de sua função de valor. Enquanto o \textit{Q-Learning} adota uma política de atualização
\textit{off-policy}, o algoritmo SARSA utiliza uma abordagem \textit{on-policy}, que 
atualiza os valores de ação com base na política atualmente em uso pelo agente. 
Isso significa que a função de valor é ajustada considerando a ação que o agente de fato escolheu seguir, 
e não necessariamente a ação ótima esperada. O valor de \(Q(s,a)\) é atualizado da seguinte forma:

\begin{equation}
    Q(s_{t}, a_{t}) := Q(s_{t}, a_{t}) + \alpha * [r_{t} + \gamma * Q(s_{t+1}, a_{t+1}) - Q(s_{t}, a_{t})]
\end{equation}

Sendo assim, o \textit{SARSA} incorpora diretamente as consequências do comportamento exploratório na atualização 
de seus valores, tornando-o mais conservador em ambientes estocásticos ou perigosos, onde a exploração 
pode levar a resultados negativos. Essa característica o torna particularmente adequado para aplicações 
em que a segurança ou a estabilidade do aprendizado são prioridades, uma vez que o agente aprende a 
evitar ações arriscadas ao observar os efeitos reais de suas escolhas durante a exploração. 
No entanto, por ser um método \textit{on-policy} e depender fortemente da política seguida durante o treinamento, 
o \textit{SARSA} não garante, em todos os casos, a convergência para a política ótima. A qualidade da política 
final depende diretamente da estratégia de exploração adotada, o que pode levar a soluções subótimas, 
especialmente se a exploração for insuficiente ou mal calibrada.

\subsection{Deep Q-Networks}
As \textit{Deep Q-Networks} (DQNs) foram propostas por \citeonline{mnih2013playingatarideepreinforcement} 
como uma extensão natural do Q-Learning, incorporando redes neurais profundas como funções aproximadoras 
da função de valor \(Q(s, a)\). Essa abordagem permite que o agente aprenda diretamente a partir de representações complexas do ambiente, 
como imagens ou vetores de alta dimensionalidade, superando as limitações da tabela tradicional.

Em uma DQN, a rede recebe como entrada o estado, \(s\) e retorna uma estimativa dos valores \(Q\)
para todas as ações possíveis naquele estado. O treinamento é realizado por meio da minimização do erro quadrático temporal 
(temporal-difference error), ajustando os pesos da rede de modo que a estimativa de \(Q\)
se aproxime da recompensa observada somada ao valor estimado do próximo estado.

Para garantir a estabilidade e convergência do aprendizado, a DQN introduz duas técnicas fundamentais: 
a introdução de uma memória que armazena experiências anteriores, o \textit{Experience Replay}, e a utilização
de uma rede auxiliar \textit{target-network}.

A primeira técnica, conhecida como Experience Replay, consiste em armazenar as experiências do agente, ou seja, tuplas da forma 
\( s_{t}, a_{t}, r_{t+1}, s_{t+1} \), em uma memória chamada \textit{replay buffer}. Em vez de utilizar as experiências mais 
recentes imediatamente após cada interação com o ambiente, como no \textit{Q-Learning} clássico, a DQN realiza o treinamento com amostras 
aleatórias retiradas dessa memória. Essa abordagem possui a vantagem de quebrar a correlação temporal entre as amostras, 
o que reduz o risco de oscilações instáveis no aprendizado,  e, aumenta a eficiência do uso das experiências, 
permitindo que cada interação seja reutilizada várias vezes durante o treinamento, otimizando o aproveitamento de dados.


A segunda técnica é a introdução da rede-alvo (\textit{target network}), uma cópia da rede Q principal, 
mas com pesos que são atualizados de forma desacoplada e mais lenta. Durante o processo de treinamento, 
os valores de referência (\textit{targets}) utilizados para atualizar a rede principal são calculados com base 
nas estimativas geradas por essa rede-alvo. Isso evita
que a rede fique tentando prever um valor futuro que está mudando 
ao mesmo tempo em que ela tenta se ajustar a ele, o que pode causar instabilidades numéricas e dificultar a convergência. 
A rede-alvo pode ser atualizada de forma periódica, copiando integralmente os pesos da rede principal a cada \(N\)
passos, ou por meio de uma atualização suave, utilizando interpolação com um fator de mistura \(\tau\) segundo a equação:
\begin{equation}
    \theta_{target} = \tau \theta_{online} + (1 - \tau)\theta_{target}
\end{equation}
\noindent onde \(\theta_{online}\) e \(\theta_{target}\) representam, respectivamente, os parâmetros da rede principal e da rede-alvo.

Apesar de seu impacto significativo no avanço do aprendizado por reforço com função de valor aproximada, 
a DQN apresenta algumas limitações importantes que motivaram o desenvolvimento de variações e métodos alternativos. 
Um dos principais problemas enfrentados pela DQN é a sua instabilidade em ambientes com recompensas esparsas ou altamente 
estocásticas, onde a propagação do sinal de recompensa pode ser lenta e difícil de capturar, especialmente quando o 
agente depende de longas sequências de ações para atingir o objetivo.

Um exemplo que ilustra bem as limitações da DQN em ambientes com recompensas esparsas ocorre na geração procedural de fases em jogos,
como no caso de  
um agente cujo objetivo é construir um labirinto jogável, posicionando paredes, caminhos e obstáculos em um tabuleiro bidimensional. 
A cada passo, o agente escolhe uma ação dentre um conjunto grande de possibilidades, como adicionar uma parede em determinada posição, 
sem receber nenhuma recompensa imediata. 
A única recompensa é fornecida ao final do processo, quando o labirinto é testado por algum algoritmo de avaliação ou outro agente.
Nesse cenário o agente precisa tomar centenas de decisões corretas ao longo da geração do labirinto para alcançar uma 
recompensa significativa, o que torna a propagação do sinal de recompensa extremamente lenta e instável. 
Pequenos erros em estágios iniciais, como bloquear uma saída ou criar caminhos redundantes, podem inviabilizar 
completamente o labirinto, mesmo que o restante da geração tenha sido adequada.
Como consequência, a DQN pode levar muito tempo para descobrir sequências de ações que levem a recompensas positivas, 
dificultando o aprendizado efetivo.

Por fim, como a DQN aprende uma função de valor e extrai a política de forma indireta (via \( max_a' Q(s_{t+1}, a')\)), 
ela não modela diretamente a política do agente, o que dificulta a incorporação de restrições, 
regularizações ou explorações estocásticas mais controladas. Em contraste, métodos baseados em política 
oferecem maior flexibilidade nesse aspecto, permitindo um aprendizado mais direto da 
política ótima, com suporte nativo à exploração probabilística e à otimização estável com garantias teóricas mais fortes.



\section{Métodos de Política por Gradiente}
Os métodos de política por gradiente se diferenciam fundamentalmente dos métodos baseados em função de valor, como o Q-Learning. 
Enquanto algoritmos de valor-ação aprendem estimativas da função \(Q(s,a)\), utilizando essas estimativas para derivar uma política 
de decisão, geralmente escolhendo ações que maximizam o valor esperado, os métodos por gradiente de política adotam uma abordagem 
direta: eles aprendem uma política estocástica ou determinística parametrizada, geralmente denotada como 
\(\pi_{\theta}(a \mid s)\), onde \(\theta\) representa os parâmetros da política (geralmente os pesos de uma rede neural).

A principal vantagem dessa abordagem reside no fato de que a política pode ser amostrada diretamente durante a interação 
com o ambiente, sem a necessidade de consultar uma função de valor explícita para determinar a melhor ação. 
Embora funções de valor, como \(V(s)\) ou \(Q(s,a)\) ainda possam ser utilizadas como ferramentas auxiliares
para estimar vantagens e reduzir a variância do gradiente, elas não são estritamente necessárias para o processo de seleção de ações.

Os métodos de política por gradiente funcionam ao computar um estimador do gradiente da política e aplicá-lo em 
um algoritmo de gradiente ascendente estocástico. O estimador de gradiente mais comumente utilizado possui a seguinte forma:
\begin{equation}
    \hat{g} = \mathbb{E}_{t} \left[ \nabla_{\theta} \log \pi_{\theta}(a_{t} \mid s_{t}) \, \hat{A}_{t} \right]  
\end{equation}

Onde \(\mathbb{E}_{t}\) é a média empírica das amostras em um lote, 
em um algoritmo que alterna entre amostragem e otimização, \(\pi_{\theta}\) é uma política estocástica parametrizada 
e \(\hat{A}_{t}\) é um estimador da função
de vantagem no instante do tempo \(t\). A função de vantagem mede o quanto melhor (ou pior) foi uma ação \(a_{t}\)
comparada com a média esperada da política naquele estado \(s_{t}\):
\begin{equation}
    A_{t} = Q(s_{t}, a_{t}) - V(s_{t})
\end{equation}
Como geralmente não temos acesso ao verdadeiro \(Q\) e \(V\)
usamos aproximações baseadas nas recompensas observadas durante os episódios

Na prática, implementações que utilizam software de diferenciação automática (como TensorFlow ou PyTorch) operam construindo uma 
função objetivo cuja derivada é justamente o estimador do gradiente da política. Este estimador é obtido por meio da diferenciação
da seguinte função:
\begin{equation}
    L_{PG}(\theta) = \hat{E}_{t} [ \log \pi_{\theta}(a_{t} | s_{t}) \times \hat{A}_t ]    
\end{equation}
  
Embora possa parecer atraente realizar várias etapas de otimização sobre essa função de perda \(L_{PG}\)
utilizando uma mesma trajetória de dados, essa prática não é bem justificada teoricamente e, na prática, 
frequentemente resulta em atualizações excessivamente agressivas da política, que prejudicam o aprendizado.


\subsection{Proximal Policy Optimization}
O algoritmo \textit{Proximal Policy Optimization} (PPO), introduzido por \citeonline{SchulmanWDRK17}, 
representa um avanço significativo nos métodos de otimização de políticas em aprendizado por reforço, 
ao oferecer um equilíbrio eficaz entre simplicidade de implementação, estabilidade de treinamento e desempenho empírico. 
Ele pertence à classe dos algoritmos de política por gradiente, mais especificamente ao grupo dos métodos \textit{on-policy}, 
que atualizam a política com base nas experiências geradas pela própria política corrente.

A principal inovação do PPO está na forma como ele realiza a otimização do gradiente da política de maneira "conservadora", 
isto é, evitando que atualizações excessivamente grandes nos parâmetros causem deterioração na performance da política. 
De maneira resumida, a ideia principal do algoritmo é que após uma atualização,
a nova política deve permanecer não muito distante da política antiga.
Em vez de aplicar diretamente o gradiente estimado, o PPO introduz uma função objetivo \textit{clipped surrogate}, definida como:
\begin{equation}
    L^{CLIP} (\theta) = \mathbb{E}_{t} \left[ min(r_{t}(\theta)\hat{A}_{t}, clip(r_{t}(\theta), 1 - \epsilon, 1 + \epsilon)\hat(A)_{t})\right]
\end{equation}
\noindent Onde \(r_{t}(\theta)\) é a razão entre a nova política com a política antiga e pode ser descrita por
\begin{equation}
    r_{t}(\theta) = \frac{\pi_{\theta}(a_{t} \mid s_{t})}{\pi_{\theta_{old}}(a_{t} \mid s_{t})}
\end{equation}
\noindent E a função de \textit{clip} significa que, se a razão \(r_{t}(\theta)\) sair do intervalo \([1-\epsilon, 1+\epsilon]\),
ela será limitada no intervalo
\begin{itemize}
    \item Se \(r_{t}(\theta) < 1 - \epsilon\), o valor é truncado para \(1 - \epsilon\)
    \item Se \(r_{t}(\theta) > 1 + \epsilon\), o valor é truncado para \(1 + \epsilon\)
\end{itemize}
Imagine que o estimador da vantagem \(\hat{A}_{t}\) é positivo. Nesse caso, \(r_{t}(\theta)\hat{A}_{t}\) 
cresce conforme \(r_{t}(\theta)\) cresce. Isso encoraja a política a dar muito mais probabilidade à ação 
\(a_{t}\). Se não houvesse limites, a política poderia mudar demais e sair do caminho ótimo.

A função \textit{clip} é muito importante nesse cenário, pois impede essas atualizações drásticas, 
mantendo as mudanças dentro de uma zona de confiança. Isso torna o PPO mais robusto e estável, 
evitando o problema de \textit{policy collapse} na qual a política aprendida converge prematuramente 
para uma estratégia sub-ótima e restrita, como por exemplo, escolher a mesma ação independentemente do estado.

De maneira geral, o algoritmo \textit{Proximal Policy Optimization} destaca-se por sua estabilidade, eficiência e 
simplicidade em comparação com outros métodos de política por gradiente. Sua principal inovação é o uso de uma 
função objetivo com clipping, que restringe grandes atualizações na política, tornando o aprendizado mais estável 
mesmo em ambientes com alta complexidade. 



\section{Aprendizado por reforço no contexto de PCG}
É importante considerar as implicações e limitações surgem ao modelar um agente
para tarefas de criação, sendo a principal, o aumento considerável do número
de variáveis. Para exemplificar isso, é possível imaginar um cenário de um labirinto.

\begin{figure}[htb]
	\caption{\label{ref-maze01}Labirinto 5x5 no contexto de um agente jogador}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/maze01.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\FloatBarrier

Na Figura \ref{ref-maze01}, o boneco em palito representa um agente de RL que irá navegar pelo labirinto,
as células em cinza representam as paredes pelo qual o agente não pode se locomover, e a bandeira
representa a posição final que o agente deve alcançar. Neste exemplo, é considerado um
espaço de ações limitado, onde o agente pode se mover em apenas quatro dimensões, para cima, para baixo,
para a direita e esquerda, totalizando quatro ações possíveis. Os possíveis estados do agente são
posições neste labirinto no formato \(S(X,Y)\), onde \(X\) e \(Y\) representam um par de coordenadas.
No exemplo da figura acima, o agente está no estado \(S(0,1)\).

O espaço de observações do agente, ou seja, todas as possíveis posições que ele pode ocupar, 
consiste no produto cartesiano do intervalo \([W, H]\), na forma \((x, y)\), onde \(x \in [0, W-1]\) 
representa a posição horizontal (coluna) e \(y \in [0, H-1]\) representa a posição vertical (linha). 
Dessa forma, o espaço de observações é definido como o conjunto de todas as posições discretas possíveis 
em uma grade bidimensional de largura \(W\) e altura \(H\), ou seja:
\[
\mathcal{O} = \{(x, y) \mid x \in [0, W-1],\ y \in [0, H-1]\}
\]
\noindent Em que \(W\) e \(H\) representam a largura e altura da grade

E isso totaliza um total de \(W \times H\) possíveis estados. No exemplo da figura acima, um labirinto com 
cinco de altura e largura irá produzir um total de 25 posições para o agente. No contexto de geração de
conteúdo, esse espaço de observações é muito maior. Para exemplificar, o mesmo exemplo usado anteriormente
foi adaptado para uma nova tarefa:

\begin{figure}[htb]
	\caption{\label{ref-maze02}Labirinto 5x5 no contexto de um agente criador}
	\begin{center}
	    \includegraphics[scale=0.5]{imagens/maze02.png}
	\end{center}
	\legend{Fonte: Elaboração própria.}
\end{figure}

\FloatBarrier

Na Figura \ref{ref-maze02}, o círculo representa a posição inicial do jogador e a bandeira 
representa a sua posição objetivo, em quanto células em branco representam
posições navegáveis enquanto células em cinza representam paredes que bloqueiam os
movimentos. É importante notar que nesse caso, não existe uma definição clara da posição
do agente gerador, pois ele pode ser representado de diversas formas diferentes.
O agente pode navegar posições aleatórias do tabuleiro e ser capaz de modificá-las ou 
pode navegar em um formato já pré-definido. Nessas abordagens, o agente tem uma posição
associada e só é capaz de modificar o valor da célula dessa posição. Ainda existem abordagens 
mais complexas que levam em consideração que o agente tem o poder de mudar a célula que quiser.

Estes tipos de percepções que um agente pode ter sobre seu ambiente precisam ser levadas em
consideração, pois impactam diretamente na performance dos algoritmos.  Supondo que um agente possa alterar qualquer 
posição do labirinto, cada ação individual corresponde a uma tripla
\((x, y, t)\) onde \(x\) e \(y\) representam as coordenadas da célula a ser modificada e \(t\) representa
o novo valor que será atribuído a essa célula. Dessa forma, o espaço de ações é composto por 
todas as combinações possíveis de coordenadas 
\((x, y)\) com os possíveis tipos \(t \in [0, A-1]\), onde \(A\) representa a quantidade total 
de tipos de valores possíveis que uma célula pode assumir. No exemplo da Figura \ref{ref-maze02}, os 
possíveis tipos de célula do labirinto são \((empty, wall, start, end)\) representando respectivamente, células vazias, 
paredes, células de início e de fim.

Assim, o espaço de ações totaliza \(W \times H \times A\) possibilidades distintas. Por exemplo, 
uma possível ação seria \((0, 1, empty)\), que modifica a célula na posição \((0,1)\), originalmente do tipo início, 
para o tipo vazio. Ainda neste mesmo exemplo, um labirinto com cinco de altura e largura e quatro
ações possíveis terá um total de \(5 \times 5 \times 4 = 100\) ações distintas possíveis,
representando todas as combinações de posições no grid com os diferentes tipos de célula 
que podem ser atribuídos. 

Em relação ao espaço de observações, o número de possibilidades cresce ainda mais. No caso da tarefa de 
geração, um agente precisa ser capaz de lidar com todas as possíveis combinações de valores das células 
do labirinto. Considerando um ambiente com \(W \times H\) células e \(A\) tipos diferentes de tiles que 
podem ser atribuídos a cada célula, o total de observações possíveis é dado por:
\[
|\mathcal{O}| = A^{(W \times H)}
\]

No exemplo mostrado na Figura \ref{ref-maze02} um labirinto com um total de \(5 \times 5\) células e com 4 tipos 
de células (\(A = 4\)), o total de estados possíveis 
é \(4^{25} = 1.125.899.906.842.624\), um número exponencialmente grande que torna o problema desafiador para 
qualquer algoritmo de aprendizado por reforço.

É importante notar esse grande aumento no espaço de ações e observações, de um agente que irá jogar para um
agente que irá gerar conteúdo. Além disso, o exemplo de labirinto mencionado é considerado um jogo simples
e fácil, devido ao escopo limitado de ações do jogador e tabuleiro pequeno, o que deixa o problema ainda mais
desafiador se considerados cenários de jogos reais.

No contexto de geração procedural de conteúdo (PCG), a aplicação direta da DQN enfrenta obstáculos significativos 
devido à alta dimensionalidade dos espaços de observação e ação envolvidos. Em ambientes nos quais o agente precisa 
decidir entre uma grande variedade de ações possíveis para modificar ou gerar elementos de um mapa, cenário ou estrutura, 
a representação tabular de valores \(Q(s, a)\) torna-se inviável, e até mesmo o uso de redes neurais profundas enfrenta 
dificuldades para generalizar de maneira eficaz. Além disso, a complexidade estrutural das observações impõe um grande desafio 
à rede para capturar padrões relevantes e manter estabilidade no treinamento. Essa combinação de fatores frequentemente 
resulta em lentidão na convergência, oscilações no aprendizado e políticas sub-ótimas. Diante disso, torna-se necessário 
considerar alternativas mais robustas, como algoritmos baseados em políticas, que lidam melhor com ambientes de alta complexidade 
e com grande variedade de decisões contínuas ou estruturadas.

